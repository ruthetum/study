# 브로커 (Broker)
카프카 애플리케이션이 설치되어 있는 서버 또는 노드를 지칭
- 하나의 프로세스라고 생각 

보통 3개 이상으로 구성하는 것을 권장

브로커의 주요 역할
- 컨트롤러
- 데이터 삭제
- 컨슈머 오프셋 저장
- 그룹 코디네이터

---

## 브로커의 주요 역할
### 컨트롤러
클러스터의 다수 브로커 중 한 대가 컨트롤러의 역할을 수행

컨트롤러는 다른 브로커들의 상태를 체크하고 브로커가 클러스터에서 제외되는 경우 해당 브로커에 존재하는 리더 파티션을 재분배
- 카프카는 지속적으로 데이터를 처리하기 때문에 비정상 상태인 브로커를 빠르게 클러스터에게 제외하는 것이 중요

만약 컨트롤러 역할을 하는 브로커에 장애가 발생하는 경우 다른 브로커가 컨트롤러 역할을 수행

### 데이터 삭제
카프카는 다른 메시징 플랫폼과 다르게 컨슈머가 데이터를 가져가도 토픽의 데이터가 삭제되지 않음
- 컨슈머나 프로듀서가 데이터를 요청할 수도 없음

오직 브로커만이 데이터를 삭제할 수 있음
- delete 옵션에 따라서 일정 시간 또는 용량에 따라 데이터 삭제
- 특수한 상황에서 compact 옵션을 설정해서 가장 최신의 메시지 키가 있는 레코드를 제외하고, 나머지 데이터 삭제

데이터의 삭제는 파일 단위로 이루어지는데 이 단위를 로그 세그먼트(log segment)라고 함
- 이 세그먼트에는 다수의 데이터가 들어 있기 때문에 일반적인 데이터베이스처럼 특정 데이터를 선별해서 삭제할 수 없음

### 컨슈머 오프셋 저장
컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해 오프셋을 커밋

커밋한 오프셋은 `__consumer_offsets` 토픽에 저장되고, 여기에 저장된 오프셋을 토대로 컨슈머 그룹은 다음 레코드를 가져가서 처리

### 그룹 코디네이터
코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할 수행

컨슈머가 컨슈머 그룹에서 빠지면 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당하여 끊임없이 데이터가 처리되도록 도움
- 이렇게 파티션을 컨슈머로 재할당하는 과정을 리밸런스(rebalance)라고 함

---

## 브로커의 역할 - 데이터 저장
카프카를 실행할 때 `config/server.properties`의 log.dir 옵션에 정의한 디렉토리에 데이터를 저장 (파일로 저장)
- 토픽 이름과 파티션 번호의 조합으로 하위 디렉토리를 생성하여 데이터를 저장

log에는 메세지와 메타데이터를 저장
- `*.index`: 메세지의 오프셋을 인덱싱한 정보를 저장
- `*.timeindex`: 메세지에 포함된 타임스탬프를 기준으로 인덱싱한 정보를 저장

> 메세지: 프로듀서가 보낸 레코드 한 개를 의미

### 로그와 세그먼트
![image](https://github.com/ruthetum/study/assets/59307414/8a9f50e3-6612-4de4-a113-f5c928ce2136)

세그먼트의 파일명은 파일에 저장된 최초의 오프셋(레코드) 번호를 이용

지정된 세그먼트 크기에 따라 파일을 생성하고, 크기가 모두 차지 않아도 지정된 시간 주기에 따라 파일을 생성
- `log.segment.bytes`: 바이트 단위의 최대 세그먼트 크기 지정. 기본 값은 1GB
- `log.roll.ms`(hours): 세그먼트가 신규 생성된 이후 다음 파일로 넘어가는 시간 주기. 기본 값은 7일

### 액티브 세그먼트
액티브 세그먼트: 가장 마지막 세그먼트 파일(현재 쓰기가 일어나고 있는 파일)

액티브 세그먼트는 브로커의 삭제 대상에 포함되지 않음

액티브 세그먼트가 아닌 세그먼트는 retention 옵션에 따라 삭제 대상으로 지정

## 세그먼트의 삭제 주기 (cleanup.policy)
> cleanup.policy는 delete 와 compact 두 가지 옵션을 지원
>
> - delete: retention.ms를 지나거나, retention.bytes 사이즈 제한을 넘어선 경우, 오래된 세그먼트를 삭
> - compact: key별로 가장 최근의 value만 저장

### cleanup.policy=delete
관련 옵션
- `retention.ms`(minutes, hours): 세그먼트를 보유할 최대 기간. 기본 값은 7일 (3일 정도로 설정하는 게 보편적)
- `retention.bytes`: 파티션당 로그 적재 바이트 값. 기본 값은 -1 (별도 지정하지 않음)
- `log.retention.check.interval.ms`: 세그먼트가 삭제 영역에 들어왔는지 확인하는 간격. 기본 값은 5분

카프카에서 데이터는 세그먼트 단위로 삭제가 발생
- 로그(레코드) 단위로 개별 삭제는 불가능

로그(레코드)의 메세지 키, 메세지 값, 오프셋, 헤더 등 이미 적재된 데이터에 대해서 수정도 불가능

따라서 데이터를 적재할 때(프로듀서) 또는 데이터를 사용할 때(컨슈머) 데이터를 검증하는 것을 권장

### cleanup.policy=compact
> key별로 가장 최근의 value만 저장

![image](https://github.com/ruthetum/study/assets/59307414/e26b4413-789d-4db7-a562-42dfc4926978)

토픽 압축(compact) 정책은 일반적으로 생각하는 zip과 같은 압축(compression)과 다른 개념

여기서 압축(compact)은 메세지 키 별로 해당 메세지 키의 레코드 중 오래된 데이터를 삭제하는 정책을 의미

따라서 삭제(delete) 정책과 다르게 일부 레코드만 삭제될 수 있음
- 압축(compact) 역시 액티브 세그먼트를 제외한 데이터가 대상

#### 테일/헤드 영역, 클린/더티 로그
![image](https://github.com/ruthetum/study/assets/59307414/8ff4ee14-b222-49a7-88dd-fb646c126434)

- 테일 영역(클린 로그): 압축 정책에 의해 압축이 완료된 레코드들. 중복 메세지 키가 없음
- 헤드 영역(더티 로그): 압축 정책 적용 전의 레코드들. 중복된 메세지 키가 존재

#### min.cleanable.dirty.ratio
데이터의 압축 시작 시점은 `min.cleanable.dirty.ratio` 옵션 값에 따라 처리

`min.cleanable.dirty.ratio` 옵션은 액티브 세그먼트를 제외한 세그먼트에 남아 있는 테일 영역의 레코드 개수와 헤드 영역의의 레코드 개수의 비율을 의미
- 예를 들어 0.5로 설정하면 테일 영역의 레코드 개수가 헤드 영역의 레코드 개수와 동일할 경우 압축이 실행
- 0.9와 같이 크게 설정하면 한번 압축을 할 때 많은 데이터가 줄어들기 때문에 압축 효과가 좋음
  - 그러나 0.9 비율에 도달할 때까지 용량을 차지하기 때문에 보관 효율은 좋지 않음
- 0.1과 같이 작게 설정하면 압축이 자주 일어나서 가장 최신 데이터만 유지할 수 있음
  - 그러나 압축이 자주 발생하기 때문에 브로커에 부담을 줄 수 있음

---

## 브로커의 역할 - 복제 (Replication) 
![image](https://user-images.githubusercontent.com/59307414/155871797-4c7bb189-bb1b-4d4f-a2f5-38107ad954f6.png)

> 위 그림에 대한 설명
> - partition : 4, replication : 3
> - 파란색 상자는 Leader partition
> - 주황색 상자는 Follower partition
> 
> - 최대로 설정할 수 있는 replication 값은 브로커 수와 동일
>   - 브로커가 4개이면 replication 값은 4보다 클 수 없음

데이터 복제는 카프카를 장애 허용 시스템(fault tolerant system)으로 동작하도록 하는 요소
- 복제의 이유는 클러스터로 묶인 브로커 중 일부에 장애가 발생하더라도 데이터를 유실하지 않고 안전하게 사용하기 위함

카프카의 데이터 복제는 파티션 단위로 이루어짐
- 토픽 자체를 복제하는 것이 아님

토픽을 생성할 때 파티션의 복제 개수(replication factor)도 같이 설정되는데 직접 옵션을 선택하지 않으면 브로커에 설정된 옵션값을 활용
- 복제 개수(replication factor)의 최솟값은 1(복제 없음)이고, 최댓값은 브로커 개수만큼 설정하여 사용할 수 있음

파티션 복제로 인해 나머지 브로커에도 파티션 데이터가 복제되므로 복제 개수만큼 저장 용량이 증가하는 단점 존재
- 그러나 복제를 통해 데이터를 안전하게 사용할 수 있는 큰 장점때문에 카프카를 운영할 때 2개 이상의 복제 개수를 정하는 것이 중요

### 브로커에 장애가 발생한 경우
브로커가 다운되면 해당 브로커 안에 있는 리더 파티션을 사용할 수 없기 때문에 팔로워 파티션 중 하나가 리더 파티션 지위를 넘겨받음(승급)
- 이를 통해 데이터가 유실되지 않고 컨슈머나 프로듀서와 데이터를 주고받도록 동작할 수 있음

운영 시에는 데이터의 종류마다 다른 복제 개수를 설정하고, 상황에 따라서는 토픽마다 복제 개수를 다르게 설정하여 운영하기도 함

데이터가 일부 유실되어도 무관하고 데이터 처리 속도가 중요하다면 1 또는 2로 설정
- metric과 같이 일부 유실되어도 상관없는 경우 1로 설정
- 금융 정보와 같이 유실이 일어나면 안 되는 데이터의 경우 복제 개수를 3으로 설정

### ISR (In Sync Replica)
![image](https://github.com/ruthetum/study/assets/59307414/e8cb0f6f-6e63-4363-a125-6d8769f37c7c)

ISR은 리더 파티션과 팔로워 파티션이 모두 싱크된 상태를 의미
- 복제 개수가 2인 토픽이라면 리더 파티션 1개와 팔로워 파티션 1개가 존재
- 리더 파티션에 0~3 까지의 오프셋이 있다면 팔로워 파티션에도 0~3 까지 오프셋이 존재해야 동기화가 완료

싱크(동기화)가 완료됐다는 의미는 리더 피티션의 모든 데이터가 팔로워 파티션에 복제된 상태를 의미

#### unclean.leader.election.enable
> 싱크(복제)가 안된 팔로워 파티션에 대한 리더 선출 여부를 설정하는 옵션
> - 토픽의 특성에 따라 다르게 설정할 수 있음

- `unclean.leader.election.enable=true`: 유실을 감수함. 복제가 안 된 팔로워 파티션을 리더로 승급
- `unclean.leader.election.enable=false`: 유실을 감수하지 않음. 해당 브로커가 복구될 때까지 중단

![image](https://github.com/ruthetum/study/assets/59307414/04e0149f-d09f-40a0-8449-7fba0b1fd6c2)

리더 파티션의 데이터를 복제하지 못한 상태에서, 싱크가 되지 않은 팔로워 파티션이 리더 파티션으로 선출되면 데이터가 유실될 수 있음

유실이 발생하더라도 서비스를 중단하지 않고, 지속적으로 토픽을 사용하고 싶다면 ISR이 아닌 팔로워 파티션을 리더로 선출하도록 설정할 수 있음

---

## 토픽과 파티션
![image](https://github.com/ruthetum/study/assets/59307414/89c095bd-61d3-4b16-9ccf-08745970f23f)

토픽은 카프카에서 데이터를 구분하기 위해 사용하는 단위
- 토픽은 1개 이상의 파티션을 소유하고 있음

파티션에는 프로듀서가 보낸 데이터들이 들어가서 저장되는데 이 데이터를 레코드(record)라고 함

파티션은 큐(queue)와 비슷한 구조, FIFO 구조로 먼저 들어간 레코드는 컨슈머가 먼저 가져감

다만 큐의 경우 데이터를 가져가면(pop) 값을 삭제하지만, 카프카에서 값을 삭제하지 않음

파티션의 레코드는 컨슈머가 가져가는 것과 별개로 관리
- 토픽의 레코드는 다양한 목적을 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러 번 가져갈 수 있음

### 토픽 생성 시 파티션이 배치되는 방법
#### 리더 파티션 배치
![image](https://github.com/ruthetum/study/assets/59307414/2cedc73e-90cc-470f-ad64-020a5ec88d3e)

#### 리더 + 팔로워 파티션 배치
![image](https://github.com/ruthetum/study/assets/59307414/4eff1d3c-5b6f-4a43-9586-54ea7b763dfd)

파티션이 5개인 토픽을 생성했을 경우 그림과 같이 0번 브로커부터 시작해서 RR(Round Robin) 방식으로 리더 파티션을 배치

카프카 클라이언트는 리더 파티션이 있는 브로커와 통신하여 데이터를 주고 받으므로 여러 브로커에 골고루 네트워크 통신을 분산
- 데이터가 특정 서버(브로커)와 통신이 집중되는 현상(hot spot)을 방지하고 선형 확장(linear scale out)을 하여 데이터가 많아지더라도 대응 가능

### 특정 브로커에 파티션이 쏠리는 현상
![image](https://github.com/ruthetum/study/assets/59307414/a53b1241-a23f-4c5c-b045-ce15d4953ca1)

특정 브로커에 파티션이 몰리는 경우 `kafka-reassign-partions.sh` 명령으로 파티션을 재분배할 수 있음
- 파티션 재설정을 하는 경우 컨슈머 리밸런싱 등 내부 작업이 유발되기 때문에 정상 운영 중에는 권장하지 않음
- 브로커를 확장할 때 파티션 분배를 위해 활용할 수도 있음

### 파티션 개수와 컨슈머 개수의 처리량
![image](https://github.com/ruthetum/study/assets/59307414/688fe9f4-ff7b-40be-ac10-a0d5befa5aca)

파티션은 카프카의 병렬처리의 핵심으로써 그룹으로 묶인 컨슈머들이 레코드를 병렬로 처리할 수 있도록 매칭

컨슈머의 처리량이 한정된 상황에서 많은 레코드를 병렬로 처리하는 가장 좋은 방법은 컨슈머의 개수를 늘려 스케일 아웃하는 방법

컨슈머 개수를 늘림과 동시에 파티션 개수도 늘리면 처리량이 증가하는 효과를 볼 수 있음

### 파티션 개수를 줄이는 것은 불가능
카프카에서 파티션 개수를 줄이는 것은 지원하지 않음
- 만약 지원을 한다고 해도 여러 브로커에 저장된 데이터를 취합하고 정려하는 복잡한 과정을 거쳐야 하기 때문에 클러스터에 영향을 줄 수 있음

한번 늘리면 줄이는 것은 불가능하기 때문에 토픽을 삭제하고 재생성하는 방법 외에는 없음
- 따라서 파티션을 늘리는 작업을 할 때는 신중히 파티션 개수를 정해야 함

---

## 레코드
![image](https://github.com/ruthetum/study/assets/59307414/7592c497-7195-4fd4-bdee-1639ea07121d)

레코드는 타임스탬프, 오프셋, 헤더, 키, 메세지 값으로 구성

프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 타임스탬프가 지정되어 저장
- 옵션에 따라 타임스탬프가 지정되어 저장

브로커에 한번 적재된 레코드는 수정할 수 없고, 리텐션 기간 또는 용량에 따라서만 삭제

### 타임스탬프
타임스탬프는 스트림 프로세싱에서 활용하기 위한 시간을 저장하는 용도로 사용
- 0.10.0.0 이후 버전부터 추가됨
- Unix timestamp 설정

프로듀서에서 따로 설정하지 않으면 기본값으로 ProducerRecord 생성 시간(CreateTime)으로 설정 또는 브로커 적재 시간(LogAppendTime)으로 설정할 수도 있음
- 토픽 단위로 설정 가능하고, `message.timestamp.type` 옵션을 통해 설정

### 오프셋
오프셋은 프로듀서가 생성한 레코드에는 존재하지 않음
- 프로듀서가 전송한 레코드가 브로커에 적재될 때 오프셋이 지정

오프셋은 0부터 시작되고 1씩 증가

컨슈머는 오프셋을 기반으로 처리가 완료된 데이터와 앞으로 처리해야할 데이터를 구분

각 메세지는 파티션별로 고유한 오프셋을 가지므로 컨슈머에서 중복 처리를 방지하기 위한 목적으로 사용

### 헤더
헤더는 key/value 데이터를 추가할 수 있으며, 레코드의 스키마 버전이나 포맷과 같이 데이터 프로세싱에 참고할만한 정보를 담아서 사용
- 0.11 부터 제공

### 키
키는 처리하고자 하는 메세지 값을 분류하기 위한 용도로 사용 (파티셔닝)

파티셔닝에 사용하는 메세지 키는 파티셔너(partitioner)에 따라 토픽의 파티션 번호가 정해짐

메세지 키는 필수 값이 아니며, 지정하지 않는 경우 null로 설정됨
- 메세지 키가 null인 레코드는 특정 토픽의 파티션에 round robin 으로 전달

null이 아닌 메세지 키는 해시 값에 의해 특정 파티션에 매핑되어 전달
- 따라서 특정 키를 설정하는 경우 순차성을 보장 (FIFO) 

### 메세지 값
메세지 값은 실질적으로 처리할 데이터가 담기는 공간

메세지 값의 포맷은 제네릭을 사용자에 의해 지정
- 다양한 형태로 지정 가능하며 필요에 따라 사용자 지정 포맷으로 직렬화/역직렬화 클래스를 만들어서 사용 가능

브로커에 저장된 레코드의 메세지 값은 어떤 포맷으로 직렬화되어 저장되어있는지 알 수 없기 때문에 컨슈머는 미리 역직렬화 포맷을 알고 있어야 함

---

## 토픽 이름 제약 조건
- 빈 문자열 토픽 이름은 지원하지 않음
- 토픽 이름은 249자 미만이여야 함
- 토픽 이름은 마침표 하나(`.`) 또는 마침표 둘(`..`)로 생성될 수 없음
- 토픽 이름은 영어 대/소문자, 숫자 0~9, 마침표(`.`), 언더바(`_`), 하이픈(`-`) 조합으로 생성 가능
- 카프카 내부 로직 관리 목적으로 사용되는 2개 토픽과 동일한 이름은 사용 불가(`__consumer_offsets`, `__transaction_state`)
- 카프카 내부적으로 사용하는 로직 때문에 마침표(`.`), 언더바(`_`)가 동시에 들어가면 안 됨
  - 생성은 할 수 있지만 사용 시 이슈가 발생할 수 있기 때문에 동시에 들어간 토픽 이름을 사용하는 경우 warning 메시지 발생

## 토픽 이름 작명 방법
토픽 이름은 어떤 용도로 누가 사용하고 있는지, 어떻게 만들어졌는지 알 수 있게 작명하는 것이 좋음

토픽 이름에 대한 규칙을 구성원간 사전에 정의하고 사용하는 것이 중요함

카프카는 토픽 이름 변경을 지원하지 않기 때문에 이름을 변경하기 위해서는 삭제 후 다시 생성하는 방법밖에 없음

### 토픽 작명 탬플릿과 예시
#### `<환경>.<팀명>.<애플리케이션명>.<메세지타입>`
- dev.marketing-team.sms-platform.json

#### `<프로젝트명>.<서비스명>.<환경>.<이벤트명>`
- commerce.payment.prod.notification

#### `<환경>.<서비스명>.<JIRA번호>.<메세지타입>`
- dev.email-sender.jira-1234.json

#### `<카프카-클러스터명>.<환경>.<서비스명>.<메세지타입>`
- kafka-cluster-1.prod.marketing-platform.json

#### ETC
- `<namespace>.<product>.<event-type>`
- `<application>-<data-type>-<event-type>`
- `<organization>.<application-name>.<event-type>.<event>`
- `<message type>.<dataset name>.<data name>`
- `<root name space>.<product>.<product specific hierarchy>`
- `<app type>.<app name>.<dataset name>.<stage of processing>`
- `<app type>.<dataset name>.<data>`

#### `<message type>.<dataset name>.<data name>`
- message type: 조직에서 정의하는 메세지 타입
  - logging, queuing, tracking, etl/db, streaming, push, user, etc

- dataset name: RDBMS의 데이터베이스(스키마) 이름과 유사, 항목을 그룹화하는 카테고리로 활용
  - marketing-platform, payment-platform, inventory-platform, etc

- data name: RDMBS의 테이블 이름과 유사
  - user, product, order, payment, inventory, etc

- cf. https://cnr.sh/essays/how-paint-bike-shed-kafka-topic-naming-conventions

## 클라이언트 메타데이터

카프카 클라이언트는 통신하고자 하는 리더 파티션의 위치를 알기 위해 데이터를 주고(프로듀서) 받기(컨슈머) 전에 메타데이터를 브로커로부터 전달받음

메타데이터는 다음과 같은 옵션을 통해 리프래쉬
- `metadata.max.age.ms`: 메타데이터를 강제로 리프래시하는 간격. 기본값은 5분
- `metadata.max.idle.ms`: 프로듀서가 유휴상태일 경우 메타데이터를 캐시에 유지하는 기간. 기본값은 5분
  - 예를 들어 프로듀서가 특정 토픽으로 데이터를 보낸 후 지정한 시간이 지나고 나면 강제로 메타데이터를 리프래시

### 클라이언트 메타데이터가 이슈가 발생한 경우
카프카 클라이언트는 반드시 리더 파티션과 통신해야 함

만약 메타데이터가 현재의 파티션 상태에 맞게 리프래시되지 않은 상태에서 잘못된 브로커로 데이터를 요청하는 경우 `LEADER_NOT_AVAILABLE` exception이 발생
- 클라이언트(프로듀서 또는 컨슈머)가 데이터를 요청한 브로커에 리더 파티션이 없는 경우 발생
- 대부분 메타데이터 리프래시 이슈로 발생
- 이 에러가 자주 발생한다면 메타데이터 리프래시 간격을 확인하고 클라이언트가 정상적인 메타데이터를 가지고 있는지 확인